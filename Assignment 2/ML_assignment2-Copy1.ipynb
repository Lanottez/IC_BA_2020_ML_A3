{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "\n",
    "data_path = \"./\"\n",
    "\n",
    "# 1. loads the data file;\n",
    "training_data = pd.read_csv(data_path + 'training.csv')\n",
    "\n",
    "validation_data = pd.read_csv(data_path + 'validation.csv')\n",
    "\n",
    "test1_data = pd.read_csv(data_path + 'test1.csv')\n",
    "\n",
    "test2_data = pd.read_csv(data_path + 'test2.csv') \n",
    "\n",
    "# 2. load txt file\n",
    "text_file = open('censored_list_test1.txt', 'r')\n",
    "list_censored_test1 = text_file.read().split()\n",
    "text_file.close()\n",
    "\n",
    "text_file = open('censored_list_test2.txt', 'r')\n",
    "list_censored_test2 = text_file.read().split()\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "\n",
    "def cleaning_sms(col):\n",
    "    # convert to lower-case\n",
    "    col = col.str.lower()\n",
    "    #remove digits\n",
    "    col= col.str.replace('\\d+', ' ')\n",
    "    #remove punctuation\n",
    "    col = col.str.replace('[^\\w\\s]',' ')\n",
    "    return col\n",
    "\n",
    "training_data[\"sms\"] = cleaning_sms(training_data[\"sms\"])\n",
    "validation_data[\"sms\"] = cleaning_sms(validation_data[\"sms\"])\n",
    "test1_data[\"sms\"]= cleaning_sms(test1_data[\"sms\"])\n",
    "test2_data[\"sms\"] = cleaning_sms(test2_data[\"sms\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3\n",
    "\n",
    "class NaiveBayesForSpam:\n",
    "#These two functions trains the  Naive Bayes classifier, i.e get the labeled SMS(messages for which we know whether they're spam or ham), \n",
    "#iterate over each individual message and identify\n",
    "\n",
    "    def train (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages)\n",
    "            self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)])\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "        \n",
    "    def train2 (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        spamkeywords = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages)\n",
    "            if prob1 * 20 < prob2:\n",
    "                self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)])\n",
    "                spamkeywords.append (w)\n",
    "        self.words = spamkeywords\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "\n",
    "#Returning predictions, bayes's theorem is applied in this function \n",
    "    def predict (self, message):\n",
    "        posteriors = np.copy (self.priors)\n",
    "        for i, w in enumerate (self.words):\n",
    "            if w in message.lower():  # convert to lower-case\n",
    "                posteriors *= self.likelihoods[:,i]\n",
    "            else:                                   \n",
    "                posteriors *= np.ones (2) - self.likelihoods[:,i]\n",
    "            posteriors = posteriors / np.linalg.norm (posteriors)  # normalise\n",
    "        if posteriors[0] > 0.5:\n",
    "            return ['ham', posteriors[0]]\n",
    "        return ['spam', posteriors[1]]    \n",
    "\n",
    "#Calcauting accuracy score and confusion matrix of the classifiers\n",
    "    def score (self, messages, labels):\n",
    "        ## | TP | FP |\n",
    "        ## | FN | TN |\n",
    "        confusion = np.zeros(4).reshape (2,2)\n",
    "        for m, l in zip (messages, labels):\n",
    "            if self.predict(m)[0] == 'ham' and l == 'ham':\n",
    "                confusion[0,0] += 1\n",
    "            elif self.predict(m)[0] == 'ham' and l == 'spam':\n",
    "                confusion[0,1] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'ham':\n",
    "                confusion[1,0] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'spam':\n",
    "                confusion[1,1] += 1\n",
    "        return (confusion[0,0] + confusion[1,1]) / float (confusion.sum()), confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "#### The functions  above are used to calculate prior probability based on Bayes theorem\n",
    "__train function__ \n",
    "* creates a set of all the words present in the spam and ham messages\n",
    "* calculates the frequency with each word is present in a sentence, this frequency represents the ratio of the times a spam/ham message contains such words. It is noted that this ratio is restricted to be lower than 0.95\n",
    "\n",
    "__train2 function__\n",
    " the function performs the same procedures as train 1 with the following differences\n",
    "* In train2 function a new list is generated named \"spamkeywords\", which contains letters with high probability to appear in spamMessage, actually those that appear more than 20 times more often in spam than in ham messages\n",
    "* Also, the prior probability(likelihood function) only contains the letters with high probability\n",
    "\n",
    "__predict function__\n",
    "* Initially all the words in message are converted to lower case \n",
    "* Then the function uses the naives bayes method to calculate the posterior probability as a proportion of likelihood and prior probability\n",
    "* After performing a normalisation, to deal with computational issues, the message is categorised as  spam or ham according to the value of the posterior probability calculated\n",
    "\n",
    "\n",
    "__score function__\n",
    "* The function aggregates the number the messages based on whether they have been categorised correctly or wrongly . Essentialy it calculates and returns the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 5\n",
    "training_ham = list(training_data[training_data.label =='ham'].sms)\n",
    "training_spam = list(training_data[training_data.label =='spam'].sms)\n",
    "\n",
    "naive_bayes_training1=NaiveBayesForSpam()\n",
    "naive_bayes_training1.train(training_ham,training_spam)\n",
    "\n",
    "naive_bayes_training2=NaiveBayesForSpam()\n",
    "naive_bayes_training2.train2(training_ham,training_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "\n",
    "The train2 is faster because it uses far less words in order to calculate the posterior probability, consequently the list of words we are comparing each word in the validation set is way shorter and far less calculations are taking place, for example comparisons and multiplications. \n",
    "\n",
    "The train2 is more accurate because it uses words that are 20+ times more often present more in spam than in ham messages. Consequently it is more probable to be more important than the classifiers that are ignored. \n",
    "Those \"non-important\" classifier might be misleading as it may cause noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we use train classifier, there are 140.0 false positive.\n",
      "If we use train2 classifier, there are 140.0 false positive.\n",
      "In order to reduce the number of false positives at the expense of false negatives we could decrease the limit at the threshold in the last part of  \"predict\" function at the line \"if posteriors[0] > 0.5:\" from 0.5 to a smaller number.\n"
     ]
    }
   ],
   "source": [
    "#Q8\n",
    "false_positives1 = confusion_matrix1[1][0][1]\n",
    "false_positives2 = confusion_matrix2[1][0][1]\n",
    "print('If we use train classifier, there are',false_positives1,'false positive.')\n",
    "print('If we use train2 classifier, there are',false_positives2,'false positive.')\n",
    "print('In order to reduce the number of false positives at the expense of false negatives we could decrease the limit at the threshold in the last part of  \"predict\" function at the line \"if posteriors[0] > 0.5:\" from 0.5 to a smaller number.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9\n",
    "We will ignore the missing variables when we predict the probability. Consequently this formula will become $P(Y = C_j | X_1 = x_1, ..., X_p = x_p) = P(Y = C_j | X_1 = x_1, ..., X_{j-1} = x_{j-1}, X_{k+1} = x_{k+1}, ..., X_p = x_p)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for list_censored_test1\n",
    "class NaiveBayesForSpam_updated_censored1_Qian:\n",
    "    #rewritten the class with changed the predict function:\n",
    "    def train (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages)\n",
    "            self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)])\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "        \n",
    "    def train2 (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        spamkeywords = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages)\n",
    "            if prob1 * 20 < prob2:\n",
    "                self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)])\n",
    "                spamkeywords.append (w)\n",
    "        self.words = spamkeywords\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "\n",
    "    def predict (self, message):\n",
    "        posteriors = np.copy (self.priors)\n",
    "        for i, w in enumerate (self.words) :\n",
    "            if (w not in list_censored_test1): # This algorithm ignores the censored that are included in test1.txt\n",
    "                if w in message.lower(): \n",
    "                    posteriors *= self.likelihoods[:,i]\n",
    "                else:                                   \n",
    "                    posteriors *= np.ones (2) - self.likelihoods[:,i]\n",
    "            posteriors = posteriors / np.linalg.norm (posteriors)  # normalise\n",
    "        if posteriors[0] > 0.5:\n",
    "            return ['ham', posteriors[0]]\n",
    "        return ['spam', posteriors[1]]    \n",
    "\n",
    "    def score (self, messages, labels):\n",
    "        ## | TP | FP |\n",
    "        ## | FN | TN |\n",
    "        confusion = np.zeros(4).reshape (2,2)\n",
    "        for m, l in zip (messages, labels):\n",
    "            if self.predict(m)[0] == 'ham' and l == 'ham':\n",
    "                confusion[0,0] += 1\n",
    "            elif self.predict(m)[0] == 'ham' and l == 'spam':\n",
    "                confusion[0,1] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'ham':\n",
    "                confusion[1,0] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'spam':\n",
    "                confusion[1,1] += 1\n",
    "        return (confusion[0,0] + confusion[1,1]) / float (confusion.sum()), confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version of 欣如\n",
    "class NaiveBayesForSpam_updated_censored1_xinru:\n",
    "    #rewritten the class with changed the predict function:\n",
    "    def train (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages)\n",
    "            self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)])\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "        \n",
    "    def train2 (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        spamkeywords = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages)\n",
    "            if prob1 * 20 < prob2:\n",
    "                self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)])\n",
    "                spamkeywords.append (w)\n",
    "        self.words = spamkeywords\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "\n",
    "    def predict (self, message):\n",
    "        posteriors = np.copy (self.priors)\n",
    "        missing = set(self.words)-set(list_censored_test1)\n",
    "        self.missing = missing\n",
    "        for i, w in enumerate (self.words):\n",
    "            if w in missing:\n",
    "                continue\n",
    "            if w in message.lower(): \n",
    "                posteriors *= self.likelihoods[:,i]\n",
    "            else:                                   \n",
    "                posteriors *= np.ones (2) - self.likelihoods[:,i]\n",
    "            posteriors = posteriors / np.linalg.norm (posteriors)  # normalise\n",
    "        if posteriors[0] > 0.5:\n",
    "            return ['ham', posteriors[0]]\n",
    "        return ['spam', posteriors[1]]    \n",
    "\n",
    "    def score (self, messages, labels):\n",
    "        ## | TP | FP |\n",
    "        ## | FN | TN |\n",
    "        confusion = np.zeros(4).reshape (2,2)\n",
    "        for m, l in zip (messages, labels):\n",
    "            if self.predict(m)[0] == 'ham' and l == 'ham':\n",
    "                confusion[0,0] += 1\n",
    "            elif self.predict(m)[0] == 'ham' and l == 'spam':\n",
    "                confusion[0,1] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'ham':\n",
    "                confusion[1,0] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'spam':\n",
    "                confusion[1,1] += 1\n",
    "        return (confusion[0,0] + confusion[1,1]) / float (confusion.sum()), confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy rate of classifer train is 0.9680933852140078\n"
     ]
    }
   ],
   "source": [
    "training_ham = list(training_data[training_data.label =='ham'].sms)\n",
    "training_spam = list(training_data[training_data.label =='spam'].sms)\n",
    "\n",
    "#train test1:\n",
    "naive_bayes_test_censored1_Qian = NaiveBayesForSpam_updated_censored1_Qian()\n",
    "naive_bayes_test_censored1_Qian.train(training_ham, training_spam)\n",
    "confusion_matrix_test_censored1_Qian = naive_bayes_test_censored1_Qian.score(test1_data.sms, test1_data.label)\n",
    "print('The accuracy rate of classifer train is',confusion_matrix_test_censored1_Qian[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy rate of classifer train is 0.888715953307393\n"
     ]
    }
   ],
   "source": [
    "training_ham = list(training_data[training_data.label =='ham'].sms)\n",
    "training_spam = list(training_data[training_data.label =='spam'].sms)\n",
    "\n",
    "#train test1:\n",
    "naive_bayes_test_censored1_xinru = NaiveBayesForSpam_updated_censored1_xinru()\n",
    "naive_bayes_test_censored1_xinru.train(training_ham, training_spam)\n",
    "\n",
    "confusion_matrix_test_censored1_xinru = naive_bayes_test_censored1_xinru.score(test1_data.sms, test1_data.label)\n",
    "print('The accuracy rate of classifer train is',confusion_matrix_test_censored1_xinru[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4122"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(naive_bayes_test_censored1_xinru.missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4540"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(naive_bayes_test_censored1_xinru.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "485"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_censored_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
