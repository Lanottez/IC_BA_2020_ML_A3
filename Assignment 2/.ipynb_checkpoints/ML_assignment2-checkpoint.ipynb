{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "\n",
    "# 1. loads the data file;\n",
    "training = pd.read_csv('training.csv')\n",
    "validation = pd.read_csv('validation.csv')\n",
    "test1 = pd.read_csv('test1.csv')\n",
    "test2 = pd.read_csv('test2.csv')\n",
    "\n",
    "# 2. load txt file\n",
    "list_censored_test1 = pd.read_table('censored_list_test1.txt',header=None,names=['words'])\n",
    "list_censored_test2 = pd.read_table('censored_list_test2.txt',header=None,names=['words'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "\n",
    "#convert training dataframe\n",
    "training[\"sms\"] = training['sms'].str.replace('[^\\w\\s]','')\n",
    "training['sms'] = training['sms'].str.replace('\\d+', '')\n",
    "training['sms'] = training['sms'].str.lower()\n",
    "\n",
    "#convert validation dataframe \n",
    "validation[\"sms\"] = validation['sms'].str.replace('[^\\w\\s]','')\n",
    "validation['sms'] = validation['sms'].str.replace('\\d+', '')\n",
    "validation['sms'] = validation['sms'].str.lower()\n",
    "\n",
    "#convert test1 dataframe \n",
    "test1[\"sms\"] = test1['sms'].str.replace('[^\\w\\s]','')\n",
    "test1['sms'] = test1['sms'].str.replace('\\d+', '')\n",
    "test1['sms'] = test1['sms'].str.lower()\n",
    "\n",
    "#convert test1 dataframe \n",
    "test2[\"sms\"] = test2['sms'].str.replace('[^\\w\\s]','')\n",
    "test2['sms'] = test2['sms'].str.replace('\\d+', '')\n",
    "test2['sms'] = test2['sms'].str.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3\n",
    "\n",
    "class NaiveBayesForSpam:\n",
    "#These two functions trains the  Naive Bayes classifier, i.e get the labeled SMS(messages for which we know whether they're spam or ham), \n",
    "#iterate over each individual message and identify\n",
    "\n",
    "    def train (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages)\n",
    "            self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)])\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "        \n",
    "    def train2 (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        spamkeywords = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages)\n",
    "            if prob1 * 20 < prob2:\n",
    "                self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)])\n",
    "                spamkeywords.append (w)\n",
    "        self.words = spamkeywords\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "\n",
    "#Returning predictions, bayes's theorem is applied in this function \n",
    "    def predict (self, message):\n",
    "        posteriors = np.copy (self.priors)\n",
    "        for i, w in enumerate (self.words):\n",
    "            if w in message.lower():  # convert to lower-case\n",
    "                posteriors *= self.likelihoods[:,i]\n",
    "            else:                                   \n",
    "                posteriors *= np.ones (2) - self.likelihoods[:,i]\n",
    "            posteriors = posteriors / np.linalg.norm (posteriors)  # normalise\n",
    "        if posteriors[0] > 0.5:\n",
    "            return ['ham', posteriors[0]]\n",
    "        return ['spam', posteriors[1]]    \n",
    "\n",
    "#Calcauting accuracy score and confusion matrix of the classifiers\n",
    "    def score (self, messages, labels):\n",
    "        confusion = np.zeros(4).reshape (2,2)\n",
    "        for m, l in zip (messages, labels):\n",
    "            if self.predict(m)[0] == 'ham' and l == 'ham':\n",
    "                confusion[0,0] += 1\n",
    "            elif self.predict(m)[0] == 'ham' and l == 'spam':\n",
    "                confusion[0,1] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'ham':\n",
    "                confusion[1,0] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'spam':\n",
    "                confusion[1,1] += 1\n",
    "        return (confusion[0,0] + confusion[1,1]) / float (confusion.sum()), confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "\n",
    "#### The two training functions in below is used to calculate prior probability for Bayes theorem in below\n",
    "train function: \n",
    "* initialise a prior belief of probability that a spam/ham message according to the percentage appearance of each words in spam/hamMessage. \n",
    "* i.e. for each words in a sentence, this probability suggests the % of time a spam/ham message contains such words.\n",
    "* this percentage is restricted to be lower than 0.95\n",
    "\n",
    "train2 function: \n",
    "* Difference between train fuction and train2 function is that: in train2 it generate a new list \"spamkeywords\", which contains letters with high possibility to appear in spamMessage.\n",
    "* Also, the prior probability(likelihood function) only contains the letters with high probability\n",
    "\n",
    "\n",
    "predict function:\n",
    "* This function is using bayes theorem to calculate posterior probability based on likelihood function and prior probability (but ignored marginal likelihood function). \n",
    "* The function calculated the probability of a message being spam/ham, them predict the type of this message according to higher probability. \n",
    "\n",
    "score function:\n",
    "\n",
    "* This function generate the likelihood table according to according to prediction result under function above. It aggrgate the number of counts of each element under messages being corrected predicted/wrongly predicted under the four classes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5\n",
    "training_ham = list(training[training.label =='ham'].sms)\n",
    "training_spam = list(training[training.label =='spam'].sms)\n",
    "\n",
    "NaiveByes_test1=NaiveBayesForSpam()\n",
    "NaiveByes_test2=NaiveBayesForSpam()\n",
    "\n",
    "\n",
    "NaiveByes_test1.train(training_ham,training_spam)\n",
    "NaiveByes_test2.train2(training_ham,training_spam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation1_miscassification_rate: 0.045\n",
      "validation2_miscassification_rate: 0.038\n",
      "validation1_accuracy_rate: 0.955\n",
      "validation2_accuracy_rate: 0.962\n",
      "validation1_sensitivity_rate: 0.793\n",
      "validation2_sensitivity_rate: 0.75\n",
      "validation1_specificity_rate: 0.981\n",
      "validation2_specificity_rate: 0.997\n"
     ]
    }
   ],
   "source": [
    "#Q6\n",
    "\n",
    "#test validation\n",
    "validation_result1=[]\n",
    "validation_result2=[]\n",
    "for i in validation.sms:\n",
    "    validation_result1.append(NaiveByes_test1.predict(i))\n",
    "    validation_result2.append(NaiveByes_test2.predict(i))\n",
    "\n",
    "# validation_1_count_ST means: the actual class is Spam and we predicted Spam (correctly)\n",
    "validation1_count_ST=0\n",
    "validation1_count_SF=0\n",
    "validation1_count_HT=0\n",
    "validation1_count_HF=0\n",
    "\n",
    "# Calculate confusoin matrix\n",
    "for i in range(len(validation.label)-1):\n",
    "    if validation.label[i]== \"spam\":\n",
    "        if validation.label[i]==validation_result1[i][0]:\n",
    "            validation1_count_ST+=1\n",
    "        else:\n",
    "            validation1_count_SF+=1\n",
    "    elif validation.label[i]== \"ham\":    \n",
    "        if validation.label[i]==validation_result1[i][0]:\n",
    "            validation1_count_HT+=1\n",
    "        else:\n",
    "            validation1_count_HF+=1\n",
    "            \n",
    "# Hence the prediction for testing1:\n",
    "validation1_miscassification_rate= (validation1_count_SF+validation1_count_HF)/len(validation.label)\n",
    "validation1_accuracy_rate= 1- validation1_miscassification_rate\n",
    "validation1_sensitivity_rate= (validation1_count_ST)/(validation1_count_ST+validation1_count_SF)\n",
    "validation1_specificity_rate= (validation1_count_HT)/(validation1_count_HT+validation1_count_HF)\n",
    "\n",
    "\n",
    "\n",
    "# validation_2_count_ST means: the actual class is Spam and we predicted Spam (correctly)\n",
    "validation2_count_ST=0\n",
    "validation2_count_SF=0\n",
    "validation2_count_HT=0\n",
    "validation2_count_HF=0\n",
    "\n",
    "# Calculate confusoin matrix\n",
    "for i in range(len(validation.label)-1):\n",
    "    if validation.label[i]== \"spam\":\n",
    "        if validation.label[i]==validation_result2[i][0]:\n",
    "            validation2_count_ST+=1\n",
    "        else:\n",
    "            validation2_count_SF+=1\n",
    "    elif validation.label[i]== \"ham\":    \n",
    "        if validation.label[i]==validation_result2[i][0]:\n",
    "            validation2_count_HT+=1\n",
    "        else:\n",
    "            validation2_count_HF+=1\n",
    "            \n",
    "# Hence the prediction for testing1:\n",
    "validation2_miscassification_rate= (validation2_count_SF+validation2_count_HF)/len(validation.label)\n",
    "validation2_accuracy_rate= 1- validation2_miscassification_rate\n",
    "validation2_sensitivity_rate= (validation2_count_ST)/(validation2_count_ST+validation2_count_SF)\n",
    "validation2_specificity_rate= (validation2_count_HT)/(validation2_count_HT+validation2_count_HF)\n",
    "\n",
    "print(\"validation1_miscassification_rate: \"+str(validation1_miscassification_rate))\n",
    "print(\"validation2_miscassification_rate: \"+str(validation2_miscassification_rate))\n",
    "\n",
    "print(\"validation1_accuracy_rate: \"+str(round(validation1_accuracy_rate,3)))\n",
    "print(\"validation2_accuracy_rate: \"+str(round(validation2_accuracy_rate,3)))\n",
    "\n",
    "print(\"validation1_sensitivity_rate: \"+str(round(validation1_sensitivity_rate,3)))\n",
    "print(\"validation2_sensitivity_rate: \"+str(round(validation2_sensitivity_rate,3)))\n",
    "\n",
    "print(\"validation1_specificity_rate: \"+str(round(validation1_specificity_rate,3)))\n",
    "print(\"validation2_specificity_rate: \"+str(round(validation2_specificity_rate,3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7: \n",
    "The train2 classifier is faster because it only saved the important spam words\n",
    "The train2 is more accurate because it use only important classifier, those non-important classifier might be misleading as it cause noise\n",
    "for example, if the calculated probability of Spam&ham use particular classifier is around half&half. Then it's better to ignore such classifier as it's meaningless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive in validation1: 739\n",
      "False positive in validation2: 104\n"
     ]
    }
   ],
   "source": [
    "#Q8\n",
    "validation_matrix_test1=NaiveByes_test1.score(validation.sms,validation.label)\n",
    "validation_matrix_test2=NaiveByes_test2.score(validation.sms,validation.label)\n",
    "print(\"False positive in validation1: \" + str(validation1_count_HF))\n",
    "print(\"False positive in validation2: \" + str(validation2_count_HF))\n",
    "\n",
    "# There are lot's false positive under training test1. \n",
    "#This means for training test1, it has a higher possibility to classify Ham message as Spam\n",
    "# This is because training test 1 is more strict compare with traing2. \n",
    "# Hence to reduce false positive, we use classifier that has a higher deviation between P(spam|classifier)&P(ham|classifier)\n",
    "# i.e. use training test2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9\n",
    "we need to remove missing data list when predicting. \n",
    "i.e. for all items from censored_test file, remove it from classifier list when doing predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesForSpam_updatedby_censored1:\n",
    "#These two functions trains the  Naive Bayes classifier, i.e get the labeled SMS(messages for which we know whether they're spam or ham), \n",
    "#iterate over each individual message and identify\n",
    "\n",
    "    def train (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages)\n",
    "            self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)])\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "        \n",
    "    def train2 (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        spamkeywords = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages)\n",
    "            if prob1 * 20 < prob2:\n",
    "                self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)])\n",
    "                spamkeywords.append (w)\n",
    "        self.words = spamkeywords\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "\n",
    "#Returning predictions, bayes's theorem is applied in this function \n",
    "    def predict (self, message):\n",
    "        posteriors = np.copy (self.priors)\n",
    " #       self.words_updated=self.words-set(list_censored_test1.words)\n",
    "        for i, w in enumerate (self.words):\n",
    "            if w not in list_censored_test1.words:\n",
    "                if w in message.lower():  # convert to lower-case\n",
    "                    posteriors *= self.likelihoods[:,i]\n",
    "                else:                                   \n",
    "                    posteriors *= np.ones (2) - self.likelihoods[:,i]\n",
    "                posteriors = posteriors / np.linalg.norm (posteriors)  # normalise\n",
    "        if posteriors[0] > 0.5:\n",
    "            return ['ham', posteriors[0]]\n",
    "        return ['spam', posteriors[1]]    \n",
    "\n",
    "#Calcauting accuracy score and confusion matrix of the classifiers\n",
    "    def score (self, messages, labels):\n",
    "        confusion = np.zeros(4).reshape (2,2)\n",
    "        for m, l in zip (messages, labels):\n",
    "            if self.predict(m)[0] == 'ham' and l == 'ham':\n",
    "                confusion[0,0] += 1\n",
    "            elif self.predict(m)[0] == 'ham' and l == 'spam':\n",
    "                confusion[0,1] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'ham':\n",
    "                confusion[1,0] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'spam':\n",
    "                confusion[1,1] += 1\n",
    "        return (confusion[0,0] + confusion[1,1]) / float (confusion.sum()), confusion\n",
    "    \n",
    "    \n",
    "\n",
    "class NaiveBayesForSpam_updatedby_censored2:\n",
    "#These two functions trains the  Naive Bayes classifier, i.e get the labeled SMS(messages for which we know whether they're spam or ham), \n",
    "#iterate over each individual message and identify\n",
    "\n",
    "    def train (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages)\n",
    "            self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)])\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "        \n",
    "    def train2 (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        spamkeywords = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages)\n",
    "            if prob1 * 20 < prob2:\n",
    "                self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)])\n",
    "                spamkeywords.append (w)\n",
    "        self.words = spamkeywords\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "\n",
    "#Returning predictions, bayes's theorem is applied in this function \n",
    "    def predict (self, message):\n",
    "        posteriors = np.copy (self.priors)\n",
    "  #      self.words_updated=self.words-set(list_censored_test2.words)\n",
    "        for i, w in enumerate (self.words):\n",
    "            if w not in list_censored_test2.words:\n",
    "                if w in message.lower():  # convert to lower-case\n",
    "                    posteriors *= self.likelihoods[:,i]\n",
    "                else:                                   \n",
    "                    posteriors *= np.ones (2) - self.likelihoods[:,i]\n",
    "                posteriors = posteriors / np.linalg.norm (posteriors)  # normalise\n",
    "        if posteriors[0] > 0.5:\n",
    "            return ['ham', posteriors[0]]\n",
    "        return ['spam', posteriors[1]]    \n",
    "\n",
    "#Calcauting accuracy score and confusion matrix of the classifiers\n",
    "    def score (self, messages, labels):\n",
    "        confusion = np.zeros(4).reshape (2,2)\n",
    "        for m, l in zip (messages, labels):\n",
    "            if self.predict(m)[0] == 'ham' and l == 'ham':\n",
    "                confusion[0,0] += 1\n",
    "            elif self.predict(m)[0] == 'ham' and l == 'spam':\n",
    "                confusion[0,1] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'ham':\n",
    "                confusion[1,0] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'spam':\n",
    "                confusion[1,1] += 1\n",
    "        return (confusion[0,0] + confusion[1,1]) / float (confusion.sum()), confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10\n",
    "#test1:\n",
    "NaiveByes_test1_censored1=NaiveBayesForSpam_updatedby_censored1()\n",
    "NaiveByes_test1_censored1.train(training_ham,training_spam)\n",
    "validation_matrix_test1_censored1=NaiveByes_test1_censored1.score(validation.sms,validation.label)\n",
    "\n",
    "#test2:\n",
    "NaiveByes_test2_censored1=NaiveBayesForSpam_updatedby_censored1()\n",
    "NaiveByes_test2_censored1.train(training_ham,training_spam)\n",
    "validation_matrix_test2_censored1=NaiveByes_test2_censored1.score(validation.sms,validation.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q11\n",
    "\n",
    "#test1:\n",
    "NaiveByes_test1_censored2=NaiveBayesForSpam_updatedby_censored2()\n",
    "NaiveByes_test1_censored2.train(training_ham,training_spam)\n",
    "validation_matrix_test1_censored2=NaiveByes_test1_censored2.score(validation.sms,validation.label)\n",
    "\n",
    "#test2:\n",
    "NaiveByes_test2_censored2=NaiveBayesForSpam_updatedby_censored2()\n",
    "NaiveByes_test2_censored2.train(training_ham,training_spam)\n",
    "validation_matrix_test2_censored2=NaiveByes_test2_censored2.score(validation.sms,validation.label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
